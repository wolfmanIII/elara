# Linee guida RAG: Dimensione Chunk e top_k per Diverse Dimensioni di Embedding

Questo documento riassume in forma ordinata e chiara le raccomandazioni per:

- **dimensione massima del chunk di testo**
- **numero massimo di risultati (top_k)** nella query di cosine similarity
- valori consigliati per differenti **dimensioni del vettore embedding**: 384, 768, 1024, 1536.

Include inoltre note operative utili per il tuo setup self‚Äëhosted (WSL2 + 16GB RAM + pgvector + Ollama).

---

# üöÄ 1. Principi generali

La qualit√† e l‚Äôefficienza del RAG dipendono soprattutto da:

- **buon chunking** (n√© troppo corto n√© troppo lungo)
- **numero appropriato di chunk restituiti** (top_k)
- **dimensione del vettore** (costo di similarity e memoria)
- **assenza di chunk "vuoti"** (problema frequente quando si spezza per caratteri fissi)

Per evitare chunk inutili ad una sola parola (es. nomi come *Malen Trast*), si consiglia:

- definire una **dimensione minima del chunk**
- un **overlap** del 10‚Äì20%
- merge dei paragrafi troppo brevi (< 300‚Äì400 caratteri)

---

# üìè 2. Tabella riassuntiva (valori consigliati)

| Dimensione Vettore | Chunk max (caratteri) | Chunk target | Chunk min | top_k consigliato |
|--------------------|------------------------|--------------|-----------|-------------------|
| **384**            | 800‚Äì1200               | ~1000        | 400       | **6**             |
| **768**            | 900‚Äì1400               | ~1200        | 400       | **5**             |
| **1024**           | 1000‚Äì1600              | ~1300‚Äì1400   | 400‚Äì500   | **4**             |
| **1536**           | 1200‚Äì1800              | ~1400‚Äì1600   | 500       | **3**             |

I valori nella colonna *target* sono quelli consigliati come default pratici.

---

# üß† 3. Note specifiche per ciascuna dimensione

## üîπ Embedding **384** (es. MiniLM)
- Molto veloce ma meno preciso.
- Meglio aumentare leggermente top_k ‚Üí **6**.
- Chunk non troppo lunghi: **1000 caratteri** √® un ottimo valore.

## üîπ Embedding **768** (es. nomic‚Äëembed‚Äëtext)
- Punto di equilibrio perfetto per self‚Äëhosting.
- Chunk pi√π robusti: **1200 caratteri**.
- top_k ideale: **5**.

## üîπ Embedding **1024** (es. mxbai‚Äëembed‚Äëlarge)
- Ottima qualit√†, costo computazionale maggiore.
- Chunk un po‚Äô pi√π ampi (1300‚Äì1400).
- top_k ridotto a **4**.

## üîπ Embedding **1536** (modelli heavy stile OpenAI)
- Costosissimi in pgvector su CPU.
- Per self-hosting ‚Üí top_k basso (**3**).
- Chunk ampi: 1400‚Äì1600 caratteri.

---

# üß© 4. Parametri di chunking consigliati per un sistema (WSL2 + 16GB RAM)

Con embedding **768** + modello chat 8B:

- **min_chunk**: 400 caratteri
- **target_chunk**: 1200 caratteri
- **max_chunk**: 1400 caratteri
- **overlap**: 200‚Äì300 caratteri
- **top_k**: 5
- **token totali contesto**: ~1200‚Äì1500 token

Questo bilancia qualit√† del retrieval e performance, evitando timeout e surriscaldamenti.  
>*per problemi del modello nomic-text-embed per Ollama 0.13.1(latest), sono stato costretto ad usare il modello bge-m3 che utilizza vettori a 1024*

---

# üõ†Ô∏è 5. Considerazioni aggiuntive

### ‚úì Evitare chunk "quasi vuoti"
- Se un paragrafo √® troppo corto (<300‚Äì400 caratteri), va unito al precedente/successivo.
- Se spezzetti a misura fissa, si aggiunge un overlap.

### ‚úì Ridurre il rumore nelle query
top_k troppo alto genera contesto troppo esteso, lento e dispersivo.

### ‚úì Bilanciare i token per l‚ÄôLLM
I modelli 7B/8B in self-hosting non amano contesti da 2500+ token.

---

# ‚úÖ 6. Valori preconfigurati consigliati (riassunto finale)

Per un setup self‚Äëhosted moderno ma non estremo (16GB RAM):

- **Embedding 768** ‚Üí *scelta raccomandata*
- **Chunk target 1200**
- **Chunk max 1400**
- **top_k = 5**
- **Overlap 200‚Äì300**

Questi valori sono adatti a documenti tecnici e qualunque tipo di conoscenza

---

# üìö 7. Best practice per pgvector

## üß© Scelta della dimensione del vettore
- Usa sempre `VECTOR(N)` come **valore fisso** nella migration.
- Anche se modelli diversi producono dimensionalit√† diverse, puoi:
  - **ridurre** vettori maggiori (troncamento)
  - **zero‚Äëpadding** per vettori pi√π piccoli
- Il modo pi√π stabile √® scegliere il formato per il proprio progetto.

### Consiglio pratico
Per self‚Äëhosting ‚Üí **VECTOR(768)** √® la scelta pi√π equilibrata.

---

## üöÄ Ottimizzazione database pgvector

### Indici raccomandati
```sql
CREATE INDEX document_chunk_embedding_hnsw
ON document_chunk USING hnsw (embedding vector_cosine_ops);
```
- HNSW √® molto pi√π veloce dell‚ÄôIVFFlat, specialmente su dataset medi (< 500k chunk).

### Quando usare IVF Flat
- Solo se hanno **milioni** di chunk.
- Richiede `REINDEX` quando si aggiungono molti dati.
- Va calibrato con `lists = 100‚Äì200`.

### Vacuum & manutenzione
```sql
VACUUM ANALYZE document_chunk;
```
- pgvector beneficia di statistiche aggiornate.

---

# üß∞ 8. Funzione di chunking utilizzata

Di seguito un algoritmo di chunking che evita chunk troppo corti e include overlap:

```php
function chunkText(string $text, int $min = 400, int $target = 1200, int $max = 1400, int $overlap = 250): array
{
    $parts = preg_split('/
{2,}/', $text); // Splitta per paragrafi
    $chunks = [];
    $buffer = '';

    foreach ($parts as $p) {
        $p = trim($p);
        if ($p === '') continue;

        // Se il paragrafo √® troppo corto ‚Üí merge
        if (strlen($p) < $min) {
            $buffer .= ($buffer ? "
" : '') . $p;
            continue;
        }

        // Se buffer + paragrafo supera max ‚Üí chiudi chunk
        if (strlen($buffer) + strlen($p) > $max) {
            if ($buffer !== '') $chunks[] = $buffer;
            $buffer = $p;
            continue;
        }

        // Aggiungi al buffer
        $buffer .= ($buffer ? "
" : '') . $p;

        // Se raggiungiamo il target ‚Üí chiudiamo il chunk
        if (strlen($buffer) >= $target) {
            $chunks[] = $buffer;
            $buffer = '';
        }
    }

    if ($buffer !== '') $chunks[] = $buffer;

    // Aggiungi overlap
    $final = [];
    for ($i = 0; $i < count($chunks); $i++) {
        $chunk = $chunks[$i];
        if ($i > 0) {
            $prev = $chunks[$i - 1];
            $chunk = substr($prev, -$overlap) . "
" . $chunk;
        }
        $final[] = $chunk;
    }

    return $final;
}
```

### Benefici di questo metodo
- Evita chunk ‚Äúcadaveri‚Äù (solo titoletti o nomi)
- Mantiene la coerenza semantica
- Inserisce un overlap che migliora drasticamente la recall
- Produce chunk di lunghezza prevedibile

---

# üîé 9. Esempi di query SQL pgvector ottimizzate

## Cosine similarity standard
```sql
SELECT id, path, text,
       1 - (embedding <=> :query_vec) AS score
FROM document_chunk
ORDER BY embedding <=> :query_vec
LIMIT 5;
```

## Con filtraggio per documento
```sql
SELECT id, text
FROM document_chunk
WHERE path = :doc
ORDER BY embedding <=> :q
LIMIT :k;
```

## Con soglia minima di score
```sql
SELECT *, 1 - (embedding <=> :v) AS score
FROM document_chunk
WHERE (1 - (embedding <=> :v)) > 0.55
ORDER BY embedding <=> :v
LIMIT 5;
```

### Nota
- Le soglie sono sensibili al modello: per embedding 768 di qualit√†, **0.55‚Äì0.60** √® un buon range.

---

# üß† 10. Prompt template consigliato per RAG

```text
Sei un assistente e DEVI rispondere esclusivamente usando il contesto sotto.
Se la risposta non √® presente nel contesto, di' che non √® disponibile.

# CONTEX
{{context}}

# DOMANDA
{{question}}

Rispondi in modo chiaro e sintetico nella lingua dell'utente.
```

### Perch√© funziona bene
- Evita hallucination
- Costringe l‚ÄôLLM a usare i chunk
- Funziona bene con modelli 7B/8B self‚Äëhosted

---

# üóÑÔ∏è 11. √à utile usare **IVFFlat + HNSW insieme?**

## ‚ùå Risposta breve
Per un sistema RAG **self‚Äëhosted**, con **16 GB di RAM** e dataset di dimensioni medio‚Äëpiccole (documentazione, lore, manuali), **NO**: usare **entrambi gli indici** sulla stessa colonna di embedding **non √® n√© necessario n√© utile**.

Un solo indice ‚Äî **HNSW** ‚Äî √® la scelta corretta nel 99% dei casi.

---

# üß© 11.1. Differenze tra IVFFlat e HNSW

## üîπ HNSW
**Ideale per:** dataset piccoli/medi (fino a milioni moderati), contesti RAG.

**Pro:**
- Ottima qualit√† dei risultati (alta recall)
- Query veloci
- Zero tuning complesso

**Contro:**
- Indice un po‚Äô pi√π pesante
- Pi√π RAM rispetto a un IVFFlat minimale

---

## üîπ IVFFlat
**Ideale per:** dataset **molto grandi** (milioni di embedding).

**Pro:**
- Scalabile su enormi volumi
- Query pi√π leggere se configurato bene

**Contro:**
- Recall pi√π bassa se `lists`/`probes` non sono calibrati
- Necessita tuning
- Richiede "REINDEX" dopo grandi batch di insert

---

# üß† 11.2. Perch√© NON usarli insieme

Avere **due indici** (HNSW + IVFFlat) sulla stessa colonna comporta:

### ‚ùó Problemi
- **Pi√π spazio su disco**
- **Planner meno prevedibile** (Postgres pu√≤ scegliere l'indice peggiore)
- **Build pi√π lente**
- **Manutenzione raddoppiata**
- **Recall instabile** se IVFFlat non √® configurato bene

### üëç In questo contesto
- Dataset non enorme
- Self‚Äëhosting in WSL2
- Performance gi√† buone con HNSW
- Nessuna necessit√† di clusterizzazione (IVFFlat)

üëâ **Conclusione:** usare entrambi √® overkill e rischia di peggiorare la qualit√†.

---

# üü¢ 11.3. Raccomandazione ufficiale per il tuo progetto

### Usa **solo HNSW**:
```sql
CREATE INDEX document_chunk_embedding_hnsw
ON document_chunk
USING hnsw (embedding vector_cosine_ops);
```

### Quando considerare IVFFlat?
Solo se:
- superi **1‚Äì2 milioni di chunk**
- e hai problemi di latenza sulla similarity
- e sei disposto a fare tuning di:
  - `lists`
  - `probes`
  - strategie di REINDEX

In qualunque altro caso ‚Üí **HNSW √® migliore, pi√π semplice e pi√π affidabile**.

---

# üß™ 11.4. Come verificare che Postgres sta davvero HNSW

```sql
EXPLAIN ANALYZE
SELECT id, text
FROM document_chunk
ORDER BY embedding <=> :q
LIMIT 5;
```

Si dovrebbe vedere qualcosa come:
```
Index Scan using document_chunk_embedding_hnsw on document_chunk
```
Se invece si vede "Seq Scan" ‚Üí manca l'indice o Postgres non lo ritiene conveniente.

---

# üîö Fine documento

