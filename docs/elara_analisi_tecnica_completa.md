# ELARA â€” Analisi Tecnica Completa

Questo documento fornisce una **visione tecnica completa, aggiornata ed esaustiva** del motore RAG ELARA, integrando e consolidando tutte le informazioni presenti negli altri documenti tecnici allegati.

Lâ€™obiettivo Ã¨ offrire una panoramica chiara, strutturata e dettagliata dellâ€™intera architettura: estrazione documenti, chunking, embedding, persistenza, ricerca vettoriale, pipeline di retrieval, prompt engineering, configurazioni AI e API.

---

# ðŸ“š Indice
- [ELARA â€” Analisi Tecnica Completa](#elara--analisi-tecnica-completa)
- [ðŸ“š Indice](#-indice)
- [1. Panoramica Generale](#1-panoramica-generale)
- [2. Architettura del Progetto](#2-architettura-del-progetto)
- [3. Pipeline di Indicizzazione](#3-pipeline-di-indicizzazione)
    - [Passi dettagliati](#passi-dettagliati)
- [4. DocumentTextExtractor](#4-documenttextextractor)
    - [Formati supportati](#formati-supportati)
    - [Normalizzazione applicata](#normalizzazione-applicata)
    - [Obiettivo](#obiettivo)
- [5. Chunking: Strategia e Implementazione](#5-chunking-strategia-e-implementazione)
    - [Parametri consigliati](#parametri-consigliati)
- [6. Modelli di Embedding](#6-modelli-di-embedding)
    - [DimensionalitÃ  tipiche](#dimensionalitÃ -tipiche)
    - [Motivazione della scelta 1024](#motivazione-della-scelta-1024)
- [7. Persistenza con PostgreSQL e pgvector](#7-persistenza-con-postgresql-e-pgvector)
  - [DocumentFile](#documentfile)
  - [DocumentChunk](#documentchunk)
- [8. Indici Vettoriali: HNSW e IVF-FLAT](#8-indici-vettoriali-hnsw-e-ivf-flat)
  - [HNSW (consigliato)](#hnsw-consigliato)
  - [IVF-FLAT (solo per dataset enormi)](#ivf-flat-solo-per-dataset-enormi)
- [9. Ricerca Vettoriale e Retrieval](#9-ricerca-vettoriale-e-retrieval)
    - [Configurazioni](#configurazioni)
- [10. ChatbotService: Flusso di Generazione Risposte](#10-chatbotservice-flusso-di-generazione-risposte)
    - [ModalitÃ  speciali](#modalitÃ -speciali)
- [11. Prompt Engineering](#11-prompt-engineering)
- [12. Backend AI: Ollama e OpenAI](#12-backend-ai-ollama-e-openai)
    - [Ollama](#ollama)
    - [OpenAI](#openai)
- [13. Controller API e Flussi REST](#13-controller-api-e-flussi-rest)
- [14. Parametri che Influenzano la QualitÃ  delle Risposte](#14-parametri-che-influenzano-la-qualitÃ -delle-risposte)
- [15. Command Disponibili](#15-command-disponibili)
    - [`app:index-docs`](#appindex-docs)
    - [`app:list-docs`](#applist-docs)
    - [`app:unindex-file`](#appunindex-file)
- [16. Considerazioni su Performance e ScalabilitÃ ](#16-considerazioni-su-performance-e-scalabilitÃ )
    - [Per dataset piccoli/medi](#per-dataset-piccolimedi)
    - [Per dataset enormi (\>1M chunk)](#per-dataset-enormi-1m-chunk)
    - [Hardware](#hardware)
- [17. Conclusione Tecnica](#17-conclusione-tecnica)

---

# 1. Panoramica Generale
ELARA Ã¨ un **motore RAG (Retrieval-Augmented Generation)** scritto in Symfony 7.3, che consente di:

- indicizzare documenti locali (PDF, MD, DOCX, ODT),
- estrarre il testo in modo pulito e normalizzato,
- suddividerlo in chunk ottimizzati,
- generare embedding vettoriali tramite modelli AI,
- memorizzarli in PostgreSQL tramite lâ€™estensione pgvector,
- recuperare i chunk piÃ¹ rilevanti rispetto a una domanda,
- costruire un contesto da passare al modello AI (Ollama o OpenAI),
- generare una risposta basata esclusivamente sulle fonti indicizzate.

ELARA separa nettamente:
- **indicizzazione** (processo offline e batch),
- **retrieval** (runtime),
- **generazione tramite LLM**.

---

# 2. Architettura del Progetto

```
src/
  AI/              # integrazione con backend AI (OpenAI, Ollama)
  Command/         # comandi CLI per indicizzazione/lista/unindex
  Controller/      # API REST
  Entity/          # DocumentFile, DocumentChunk
  Middleware/      # ricostruzione indici vettoriali
  Repository/      # ricerca vettoriale+
  Service/         # ChatbotService, DocumentTextExtractor, ChunkingService
```

Principi chiave:
- modularitÃ ,
- astrazione del backend AI tramite AiClientInterface,
- separazione tra fase di indexing e fase di query,
- persistenza chiara, auditabile e con versioning naturale tramite DocumentFile.

---

# 3. Pipeline di Indicizzazione
Il processo segue il flusso:

**FILE â†’ Estrattore â†’ Chunking â†’ Embedding â†’ DB â†’ Indice vettoriale â†’ Query**

### Passi dettagliati
1. **Individuazione file** tramite `app:index-docs`.
2. **Creazione/aggiornamento DocumentFile**, con hash SHA1 per evitare duplicati.
3. **Rimozione chunk esistenti** per quel documento.
4. **Estrazione testo** tramite DocumentTextExtractor.
5. **Chunking** con regole min/max/overlap.
6. **Embedding** per ogni chunk tramite AiClientInterface.
7. **Persistenza** dei chunk con vettori.
8. **Ricostruzione indice vettoriale** (HNSW o IVF-FLAT).

---

# 4. DocumentTextExtractor
Si occupa di trasformare formati eterogenei in testo pulito e coerente.

### Formati supportati
- PDF (Smalot\PdfParser)
- Markdown
- DOCX (PhpOffice)
- ODT (ZipArchive + parsing XML)

### Normalizzazione applicata
- rimozione emoji,
- rimozione whitespace multipli,
- rimozione caratteri invisibili,
- correzione spazi mancanti (regex specializzate),
- trim globale.

### Obiettivo
Fornire un testo stabile, lineare, coerente e adatto alla fase di chunking.

---

# 5. Chunking: Strategia e Implementazione
Il chunking Ã¨ uno degli elementi piÃ¹ critici per un RAG.
ELARA implementa un **chunking intelligente**, che:
- evita chunk troppo corti,
- rispetta un limite massimo di sicurezza (hard max 1500 chars),
- costruisce chunk tramite frasi e parole,
- aggiunge overlap tra chunk basato su numero di caratteri,
- unisce lâ€™ultimo chunk se troppo corto.

### Parametri consigliati
- `min`: 400â€“500
- `target`: 1200â€“1600
- `max`: 1500â€“1800
- `HARD_MAX_CHARS`: 1500
- `overlap`: 200â€“300

---

# 6. Modelli di Embedding
Gli embedding rappresentano il significato del testo come vettori.

ELARA supporta qualsiasi modello embedding via AiClientInterface.

### DimensionalitÃ  tipiche
- **384** â€“ leggero, meno preciso
- **768** â€“ bilanciato
- **1024** â€“ alta qualitÃ  (es. bge-m3)
- **1536** â€“ qualitÃ  massima per sistemi piÃ¹ potenti

### Motivazione della scelta 1024
Il modello nomic-text-embed presenta bug noti su Ollama; bge-m3 produce embedding multidimensionali stabili a 1024.

---

# 7. Persistenza con PostgreSQL e pgvector
I chunk vengono memorizzati in due tabelle:

## DocumentFile
- UUID
- path
- hash
- createdAt
- relazione OneToMany verso chunk

## DocumentChunk
- UUID
- indice
- contenuto
- embedding `vector(1024)`
- relazione ManyToOne verso DocumentFile

---

# 8. Indici Vettoriali: HNSW e IVF-FLAT
Due possibili indici vettoriali, entrambi supportati da pgvector.

## HNSW (consigliato)
- Alta precisione
- Ottime performance
- Zero tuning

## IVF-FLAT (solo per dataset enormi)
- Richiede tuning (lists/probes)
- Necessita REINDEX dopo grandi batch
- Recall piÃ¹ bassa se configurato male

ELARA supporta entrambi ma **sconsiglia vivamente lâ€™uso simultaneo**.

---

# 9. Ricerca Vettoriale e Retrieval
Il retrieval avviene ordinando i chunk per similaritÃ  cosine:

```
ORDER BY embedding <=> :query_vec
```

### Configurazioni
- top_k variabile in base al modello embedding:
  - 384 â†’ 6
  - 768 â†’ 5
  - 1024 â†’ 4
  - 1536 â†’ 3

- soglia di similaritÃ  tipica: `> 0.55`

Risultati piÃ¹ simili costituiscono il **contesto** per la fase di generazione.

---

# 10. ChatbotService: Flusso di Generazione Risposte
Funzione `ask()`:
1. embed della domanda
2. ricerca vettoriale con top_k
3. costruzione contesto RAG
4. generazione prompt
5. chiamata al backend AI
6. fallback in caso di errore

### ModalitÃ  speciali
- **test mode** â†’ ricerca testuale LIKE sui chunk
- **offline fallback** â†’ estratti di testo se lâ€™AI Ã¨ indisponibile

---

# 11. Prompt Engineering
Il prompt del system Ã¨ fondamentale:

```
Rispondi SOLO usando il contesto. Non inventare.
```

Il template RAG:
```
# CONTEXT
{{context}}

# DOMANDA
{{question}}
```

Questo riduce hallucination e mantiene coerenza.

---

# 12. Backend AI: Ollama e OpenAI
ELARA permette di cambiare backend tramite ENV:

```
AI_BACKEND=ollama|openai
```

### Ollama
- locale
- nessun costo
- latenza maggiore

### OpenAI
- qualitÃ  superiore
- costi per token

Modelli chat consigliati: 7Bâ€“8B per uso locale.

---

# 13. Controller API e Flussi REST
Endpoint principale:

```
POST /api/chat
{
  "message": "domanda"
}
```

Flusso:
Client â†’ Controller â†’ ChatbotService â†’ AI â†’ Risposta JSON.

Errori gestiti:
- messaggio vuoto
- backend indisponibile
- fallback

---

# 14. Parametri che Influenzano la QualitÃ  delle Risposte
Tutti i dettagli tecnici sono riportati nel documento dedicato, ma riportiamo i punti chiave:

- modello embedding
- modello chat
- chunking (min/max/overlap)
- top_k
- soglia cosine
- indice vettoriale (HNSW consigliato)
- system prompt
- qualitÃ  del testo estratto

---

# 15. Command Disponibili
### `app:index-docs`
Indicizza o reindicizza documenti.

### `app:list-docs`
Mostra documenti indicizzati, hash, numero chunk.

### `app:unindex-file`
Rimuove un documento e i suoi chunk.

---

# 16. Considerazioni su Performance e ScalabilitÃ 
### Per dataset piccoli/medi
- HNSW Ã¨ perfetto
- embedding 768â€“1024
- modelli chat 7Bâ€“8B

### Per dataset enormi (>1M chunk)
- considerare IVF-FLAT con tuning
- ridurre dimensionalitÃ  vettori
- limitare overlap

### Hardware
- CPU-only â†’ evitare modelli troppo grandi
- 16 GB RAM â†’ limite operativo ideale 1024-dim + chat 7B

---

# 17. Conclusione Tecnica
ELARA Ã¨ un motore RAG completo, modulare, estensibile, interamente self-hosted, compatibile con PostgreSQL/pgvector e backend AI intercambiabili.

La struttura chiara e separata in fasi garantisce:
- controllo totale del flusso,
- prestazioni prevedibili,
- qualitÃ  delle risposte basata sui documenti,
- facilitÃ  di manutenzione e scaling.

---
