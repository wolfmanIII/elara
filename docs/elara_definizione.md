# 1. Cos‚Äô√® un sistema RAG, in parole semplici
Quando parliamo di RAG, parliamo di Retrieval-Augmented Generation.
Detto in modo meno tecnico: √® un sistema che unisce due mondi:
* da una parte un motore di ricerca intelligente sui miei documenti
* dall‚Äôaltra un assistente tipo chatbot che sa scrivere risposte in linguaggio naturale

Il flusso, semplificando, √® questo:
1. Raccolta delle fonti  
Carichiamo file, manuali, PDF, documentazione interna. Il sistema li legge, li pulisce, li spezzetta in piccoli pezzi di testo gestibili (i ‚Äúchunk‚Äù).

2. Indicizzazione intelligente  
Ogni pezzo di testo viene trasformato in una rappresentazione numerica che ne cattura il significato (gli embedding, ci torno tra poco). Questi numeri vengono salvati in un database che gestisce un tipo ti campo particolare(vector), che permette di confrontare ‚Äúquanto due testi si somigliano‚Äù invece di limitarsi alle parole esatte.

3. Quando l‚Äôutente fa una domanda  
Anche la domanda viene trasformata nello stesso tipo di numeri. Il sistema cerca, tra tutti i pezzi di testo indicizzati, quelli che ‚Äúsomigliano di pi√π‚Äù alla domanda, cio√® che parlano pi√π o meno delle stesse cose.

4. Risposta del chatbot  
Solo a questo punto entra in gioco il modello di AI generativa: gli passiamo la domanda + i pezzi di documentazione pi√π rilevanti e gli chiediamo:
‚ÄúRispondi usando solo queste informazioni, senza inventare.‚Äù

Il vantaggio rispetto a un ‚Äúchatbot generico‚Äù √® che:
* risponde in base alle mie fonti interne
* pu√≤ citare o riassumere documenti
* riduce molto il rischio di risposte inventate (hallucination)

In pratica RAG significa: ‚ÄúAI che risponde, ma con il naso dentro i miei documenti, non solo nella sua memoria addestrata.‚Äù
# 2. Lo Scopo di ELARA
ELARA √® il nome che abbiamo dato al motore RAG interno:
E.L.A.R.A. ‚Äî Embedding Linking & Retrieval Answering.

Il suo Scopo √® molto chiaro:

*Permettere a chiunque, senza competenze tecniche, di interrogare la documentazione disponibile semplicemente parlando con un chatbot.*

Dal punto di vista di chi lo usa, l‚Äôesperienza ideale √®:
* apro un‚Äôinterfaccia (domani web, oggi gi√† via API)
* scrivo una domanda in italiano normale
* ottengo una risposta che:
* √® leggibile
* √® basata sui nostri documenti
* e, quando serve, mi indica anche da dove arrivano le informazioni

Dietro le quinte ELARA fa un lavoro piuttosto sofisticato, ma nascosto all‚Äôutente:
* Indicizza i documenti che mettiamo in una cartella dedicata (/var/knowledge)
* Li converte in testo pulito, rimuovendo formattazioni strane, emoji e rumore
* Li spezza in ‚Äúchunk‚Äù e genera per ognuno l‚Äôembedding, cio√® la rappresentazione numerica del significato
* Salva tutto in PostgreSQL, usando l‚Äôestensione pgvector e un indice **HNSW** | IVF-FLAT per mantenere le ricerche veloci anche con molti documenti
* Quando arriva una domanda via API /api/chat, la passa al ChatbotService, che:
* crea l‚Äôembedding della domanda
* cerca i chunk pi√π simili
* costruisce un contesto
* chiama il modello AI (Ollama, OpenAI, Gemini)
* restituisce una risposta JSON pulita

La filosofia di ELARA √®:
* semplicit√† d‚Äôuso: dall‚Äôesterno √® solo ‚ÄúPOST /api/chat con un messaggio‚Äù
* flessibilit√†: si pu√≤ cambiare backend AI (locale con Ollama, cloud con OpenAI o Gemini) via configurazione, senza toccare il codice applicativo
* controllo: l‚Äôindice √® nel nostro database, i flussi sono chiari e documentati, e possiamo decidere cosa indicizzare e cosa no.
# 3. Cosa si intende per ‚Äúembedding del testo‚Äù
Adesso arriviamo al concetto chiave: embedding.

Immagina di prendere una frase, ad esempio:

> ‚ÄúCome funziona l‚Äôindicizzazione dei documenti in ELARA?‚Äù

Un modello di AI non memorizza questa frase come stringa di caratteri, ma la traduce in una lunga lista di numeri, ad esempio 1024 numeri decimali.  
Questa lista di numeri √® l‚Äôembedding della frase.

Cosa rappresenta?
* Ogni testo diventa un ‚Äúpunto‚Äù in uno spazio matematico ad alta dimensione
* Testi che parlano di cose simili finiscono vicini
* Testi che parlano di argomenti diversi finiscono lontani

√à un po‚Äô come creare una mappa invisibile delle idee: invece di guardare alle parole precise, guardiamo alla somiglianza di significato.

Nel nostro caso:
* quando indicizziamo i documenti, per ogni chunk salviamo:
  * il testo
  * il file di origine
  * e il suo embedding in una colonna vector(1024) di PostgreSQL
* quando arriva una domanda:
  * creiamo l‚Äôembedding della domanda
  * chiediamo al database: ‚ÄúQuali chunk hanno un embedding pi√π vicino a questo?‚Äù
 
 In pratica, embedding del testo = ‚Äútradurre le frasi in numeri in modo che la distanza tra i numeri rispecchi la vicinanza di significato tra i testi‚Äù.
# 4. Come lavorano insieme PostgreSQL, pgvector, IVF-FLAT, HNSW e Doctrine
Qui entriamo un po‚Äô pi√π nel tecnico, ma sempre a livello concettuale.

## PostgreSQL
√à il nostro database relazionale ‚Äúclassico‚Äù: tabelle, righe, colonne.

## pgvector   
[Github](https://github.com/pgvector/pgvector)  
[Neon Docs](https://neon.com/docs/extensions/pgvector)

√à un‚Äôestensione di PostgreSQL che aggiunge:
* un tipo di colonna vector(N), cio√® un array di N numeri
* operatori e funzioni per confrontare vettori (distanze, similarit√†)
* indici speciali per rendere queste ricerche molto pi√π veloci

Nel nostro schema:
* la entity DocumentChunk di Doctrine ha una propriet√† embedding mappata come tipo vector con lunghezza 1024
* questo campo √® ‚Äúil posto‚Äù dove salviamo l‚Äôembedding di ogni pezzo di testo

**Doctrine (in Symfony)**
Doctrine √® lo strato che collega il mondo PHP/oggetti alle tabelle Postgres.
Per ELARA:
* definiamo le entity DocumentFile e DocumentChunk
* configuriamo il tipo custom vector per pgvector
* registriamo funzioni DQL come cosine_similarity per poter scrivere query ‚Äúad alto livello‚Äù che sotto si traducono in SQL con gli operatori vettoriali di pgvector

## Idici
### IVF-FLAT e HNSW
Indici per velocizzare le operazioni di retrieve degli embedding sul database.
### **IVF-FLAT**  
[Github](https://github.com/pgvector/pgvector#ivfflat)  
[Google Cloud](https://cloud.google.com/blog/products/databases/faster-similarity-search-performance-with-pgvector-indexes)

Viene usato un algoritmo di k-means semplificato e molto pi√π leggero, se volete saperne di pi√π:  
[K-means](https://it.wikipedia.org/wiki/K-means)  
Tutto sommato √® solo un po di matematica ü§£ü§£ü§£

IVF-FLAT √® un tipo di indice per ricerche ‚Äúper somiglianza‚Äù su vettori:
* invece di confrontare ogni vettore con tutti gli altri (lentissimo su tante righe)
* crea dei cluster di vettori (liste invertite) attorno a dei ‚Äúcentri‚Äù
* quando fai una ricerca:
  * trova i cluster pi√π vicini al vettore della domanda
  * cerca solo dentro quei cluster, non su tutta la tabella

√à un po‚Äô come dividere una biblioteca in sezioni: se cerco un libro di informatica, non vado a controllare gli scaffali di cucina.

Nel nostro progetto:
* creiamo un indice IVF-FLAT sulla colonna embedding della tabella document_chunk
* usiamo un middleware (PgvectorIvfflatMiddleware) per impostare il numero di ‚Äúsonde‚Äù (quanti cluster visitare), bilanciando velocit√† e precisione

In generale l'indice IVF-FLAT √® ideale per dataset di grandi dimensioni, ma richiede tuning e pi√π manutenzione.

### **HSWM**  
[Github](https://github.com/pgvector/pgvector#hnsw)  
HNSW significa Hierarchical Navigable Small World.
√à un tipo di indice per cercare velocemente i vettori (embedding) salvati in PostgreSQL tramite l‚Äôestensione pgvector.

L‚Äôindice HNSW costruisce una sorta di mappa a livelli dove:
* in alto ci sono pochi vettori ‚Äúgenerali‚Äù
* pi√π scendi, pi√π la mappa √® dettagliata
* i collegamenti sono pensati per arrivare velocemente ai vettori pi√π simili

Quando faccio una domanda:
* si parte dal livello pi√π alto della ‚Äúmappa‚Äù
* si scende collezionando i nodi pi√π promettenti
* alla fine si ottengono rapidamente i vettori pi√π simili

√à come cercare un libro in una biblioteca gi√† ordinata, invece che sfogliare ogni libro uno per uno.

> HNSW √® un indice che permette a pgvector di trovare rapidamente gli embedding pi√π simili alla mia domanda, rendendo il motore RAG veloce, preciso e scalabile(il tutto √® sempre relativo al hardware che si ha a disposizione).

In generale l'indice HMSW √® ideale per data set di piccole e medie dimensioni, non richiede nessun tuning.

***Attenzione, non √® utile avere due indici vettoriali diversi sullo stesso campo, √® solo uno spreco di spazio e di tempo in scrittura.***

> ***Gli indici vettoriali IVF-FLAT e HNSW, sono da considerare mutualmente esclusivi***

# 5. L‚Äôoperatore vettoriale <=> di Postgres/pgvector
L‚Äôoperatore <=> √® una delle funzionalit√† che pgvector aggiunge a PostgreSQL.

In pratica:
* prende due vettori come input (ad esempio: embedding del chunk, embedding della domanda)
* calcola la cosine distance, cio√® una misura di quanto i due vettori puntano nella stessa direzione

Tradotta in parole:
* ogni embedding √® come una freccia in uno spazio astratto
* se le frecce puntano nella stessa direzione ‚Üí testi con significato simile
* se puntano in direzioni molto diverse ‚Üí testi che parlano d‚Äôaltro

L‚Äôoperatore <=> restituisce un numero:
* pi√π √® piccolo, pi√π i testi sono simili
* pi√π √® grande, pi√π sono diversi

Esempio tipico di query:
```sql
SELECT id, contenuto
FROM document_chunk
ORDER BY embedding <=> :embedding_domanda
LIMIT 5;
```
Questa query dice al database:

> ‚ÄúDammi i 5 pezzi di testo il cui significato √® pi√π vicino al significato della domanda.‚Äù

Gli indici IVF-FLAT e HNSW accelerano proprio questa operazione per arrivare rapidamente ai vettori pi√π vicini. 

Nel codice di ELARA, quando usiamo la funzione cosine_similarity in DQL, sotto sotto Doctrine traduce in SQL usando gli operatori e le funzioni di pgvector; il concetto per√≤ √® esattamente questo: ordinare i chunk dal pi√π simile al meno simile in base a <=> / cosine.

## Il flusso completo
1. Indicizzazione (command app:index-docs)
   * DocsIndexer guida tutta la pipeline di indicizzazione
   * DocumentTextExtractor estrae il testo dai file (PDF, MD, DOCX, ODT)
   * il testo viene ripulito e spezzato in chunk
   * per ogni chunk chiamiamo l‚ÄôAiClient per ottenere l‚Äôembedding
   * Doctrine salva i chunk con il loro embedding in PostgreSQL
   * l‚Äôindice **HNSW** | IVF-FLAT viene creato/aggiornato
2. Domanda utente (API /api/chat)
   * ChatbotService crea l‚Äôembedding della domanda
   * con il repository DocumentChunkRepository esegue una query ‚Äútop K simili‚Äù
   * la query ordina i chunk in base alla distanza vettoriale (cosine)
   * i primi N chunk formano il contesto da passare al modello AI
   * l‚ÄôAI genera la risposta, che viene restituita in JSON

## Perch√© un embedding di 1536 dimensioni? Perch√© √® cos√¨ importante?
Quando diciamo che un embedding √® composto da 1536 numeri, pu√≤ sembrare un dettaglio arbitrario o puramente tecnico.
In realt√† √® una scelta fondamentale per la qualit√† delle risposte del motore RAG, e vale la pena spiegarlo in modo semplice.
### 1. Ogni numero rappresenta una ‚Äúsfumatura di significato‚Äù
Un embedding √® una rappresentazione matematica del significato di un testo.  
Pi√π dimensioni ci sono, pi√π dettagli riesce a catturare.
* Con poche dimensioni (ad esempio 50 o 100), il modello riesce a ‚Äúcapire‚Äù solo concetti molto generici.
* Con molte dimensioni (oltre 1000), riesce a distinguere concetti simili, contesti, ruoli, gerarchie, relazioni, sinonimi.

Immagina lo spazio dei significati come una stanza:
* poche dimensioni = stanza piccola, dove gli oggetti si schiacciano uno contro l‚Äôaltro
* 1536 dimensioni = una ‚Äústanza‚Äù grandissima, dove ogni concetto pu√≤ trovare la sua posizione precisa
### 2. 1536 √® la dimensione ottimale dei modelli embedding moderni
Molti dei migliori modelli di embedding attuali ‚Äî come OpenAI text-embedding-3-small o analoghi modelli open-source ‚Äî generano vettori proprio di 1536 dimensioni.

Perch√©?
* √à un compromesso eccellente tra precisione semantica e costi computazionali.
* Con dimensioni inferiori gli embedding diventano meno precisi e peggiora la ricerca dei chunk pertinenti.
* Con dimensioni molto superiori aumentano i costi, il peso su PostgreSQL e i tempi di ricerca, senza un grande miglioramento in qualit√†.

1536 √®, nella pratica, lo standard attuale per un embedding general-purpose ad alta qualit√†.

### 3. Garantisce recuperi molto pi√π accurati

Ricordiamo che lo scopo principale dell‚Äôembedding √® trovare i pezzi di testo pi√π simili alla domanda.  
Con un embedding ad alta dimensionalit√†:
* due testi simili finiscono molto vicini nello spazio vettoriale
* due testi diversi sono chiaramente separati

Questo si traduce nel fatto che ELARA recupera chunk pi√π pertinenti e quindi produce risposte pi√π utili e meno generiche.

Un embedding piccolo, invece, tenderebbe a confondere concetti diversi:
* ‚Äúprocedura di sicurezza‚Äù somiglierebbe troppo a ‚Äúprocedura operativa‚Äù
* ‚Äúattivazione API‚Äù sarebbe confusa con ‚Äúattivazione operatore‚Äù
* ‚Äúutente amministratore‚Äù risulterebbe simile a ‚Äúutente generico‚Äù
Con 1536 dimensioni invece queste sfumature vengono mantenute e separate.

### 4. √à la dimensione ideale per l‚Äôoperatore vettoriale <=> e gli indici IVF-FLAT
L‚Äôaccuratezza del confronto vettoriale dipende molto dalla qualit√† degli embedding:
* l‚Äôoperatore <=> (cosine distance) funziona meglio su spazi ricchi e ben rappresentati
* l‚Äôindice IVF-FLAT riesce a clusterizzare in modo efficace solo se i vettori hanno sufficiente dimensionalit√†.
* Doctrine, pgvector e PostgreSQL lavorano in modo ottimale con questi vettori ‚Äúdensi‚Äù ad alta qualit√†

In pratica:
> pi√π qualit√† nell‚Äôembedding = meno errori nella ricerca dei chunk = risposte migliori.
### 5. √à uno standard ben supportato e pensato per la compatibilit√† futura
Scegliere una dimensione standard come 1536 significa:
* poter cambiare modello embedding in futuro senza cambiare database
* compatibilit√† con centinaia di modelli attuali e futuri
* stabilit√† dell‚Äôindice pgvector/IVF-FLAT
* pieno supporto da parte delle librerie AI usate da ELARA (sia Ollama che OpenAI che Gemini)
In altre parole: √® una scelta che protegge l‚Äôinvestimento tecnico e mantiene il sistema flessibile nel tempo.

# 6. Conclusioni: perch√© conviene un motore RAG interno

Chiudo con qualche punto:
1. Conoscenza accumulata davvero sfruttata
    I documenti non restano ‚Äúmorti‚Äù in una cartella o in un wiki: diventano una base di conoscenza a cui si accede in linguaggio naturale, via chatbot.
2. Risposte contestualizzate e aggiornabili
    Se si aggiorna una procedura o un manuale e re-indicizzi i documenti, il motore RAG inizier√† a rispondere usando le informazioni nuove, senza dover ri-addestrare un modello.
3. Controllo su dati e fonti
    Gli embedding e i testi nel PostgreSQL in locale, con pgvector e gli indici configurati e personalizzati. Si pu√≤ decidere cosa indicizzare, cosa escludere, come strutturare gli accessi e in futuro agganciare autenticazione e ruoli.
4. Privacy e conformit√†
    Si possono usare backend locali (Ollama) per evitare che i documenti escano all'esterno, oppure usare OpenAI o Gemini solo per la parte di generazione, mantenendo comunque in casa l‚Äôindice vettoriale e i dati sensibili.
5. Integrazione con lo stack esistente
    ELARA √® scritto in Symfony, usa Doctrine e PostgreSQL: si integra bene con applicazioni gi√† esistenti.
6. Scalabilit√† graduale
    Si parte da pochi documenti, cresce nel tempo; gli indici vettoriali come HNSW, oppure IVF-FLAT e le sonde configurabili se si ha un numero di chunk che super il milione, permettono di bilanciare prestazioni e qualit√† senza stravolgere l‚Äôarchitettura.

In sintesi:  
un motore RAG interno come ELARA trasforma la documentazione in un assistente consultabile in linguaggio naturale, mantenendo per√≤ controllo, trasparenza e integrazione con l‚Äôinfrastruttura esistente.
