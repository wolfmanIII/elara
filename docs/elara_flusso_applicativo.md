# ELARA â€” Flusso Applicativo Completo e Aggiornato

Lâ€™obiettivo Ã¨ offrire una descrizione chiara, completa e strutturata del **flusso applicativo reale di ELARA**, dal caricamento dei documenti fino alla generazione della risposta.

---

# ðŸ“š Indice
- [ELARA â€” Flusso Applicativo Completo e Aggiornato](#elara--flusso-applicativo-completo-e-aggiornato)
- [ðŸ“š Indice](#-indice)
- [1. Panoramica generale](#1-panoramica-generale)
- [2. Schema del flusso applicativo](#2-schema-del-flusso-applicativo)
- [3. Fase 1 â€” Acquisizione e gestione dei file](#3-fase-1--acquisizione-e-gestione-dei-file)
- [4. Fase 2 â€” Estrazione e normalizzazione del testo](#4-fase-2--estrazione-e-normalizzazione-del-testo)
  - [Formati supportati](#formati-supportati)
  - [Normalizzazioni applicate](#normalizzazioni-applicate)
- [5. Fase 3 â€” Chunking avanzato](#5-fase-3--chunking-avanzato)
    - [Caratteristiche principali](#caratteristiche-principali)
    - [Parametri consigliati](#parametri-consigliati)
- [6. Fase 4 â€” Generazione embedding](#6-fase-4--generazione-embedding)
- [7. Fase 5 â€” Persistenza e struttura dati](#7-fase-5--persistenza-e-struttura-dati)
  - [DocumentFile](#documentfile)
  - [DocumentChunk](#documentchunk)
- [8. Fase 6 â€” Costruzione o aggiornamento dellâ€™indice vettoriale](#8-fase-6--costruzione-o-aggiornamento-dellindice-vettoriale)
  - [âœ” HNSW (consigliato)](#-hnsw-consigliato)
  - [âœ” IVF-FLAT (solo dataset enormi)](#-ivf-flat-solo-dataset-enormi)
- [9. Fase 7 â€” Workflow API /api/chat](#9-fase-7--workflow-api-apichat)
- [10. Fase 8 â€” Retrieval e costruzione del contesto](#10-fase-8--retrieval-e-costruzione-del-contesto)
    - [Obiettivo](#obiettivo)
- [11. Fase 9 â€” Generazione della risposta tramite LLM](#11-fase-9--generazione-della-risposta-tramite-llm)
- [12. Fallback: Test mode \& Offline mode](#12-fallback-test-mode--offline-mode)
  - [Test mode](#test-mode)
  - [Offline fallback](#offline-fallback)
- [13. Flusso completo riassunto](#13-flusso-completo-riassunto)
- [14. Conclusione](#14-conclusione)

---

# 1. Panoramica generale
ELARA implementa un sistema completo di **Retrieval-Augmented Generation (RAG)**, in cui:
- i documenti vengono indicizzati in forma strutturata,
- le informazioni vengono trasformate in embedding vettoriali,
- PostgreSQL + pgvector effettua ricerche semantiche,
- un modello LLM (Ollama o OpenAI) genera risposte basate sui documenti.

Lâ€™obiettivo Ã¨ garantire risposte **accurate, contestualizzate e riproducibili**, senza allucinazioni.

---

# 2. Schema del flusso applicativo

```
FILE â†’ Estrattore â†’ Chunking â†’ Embedding â†’ PostgreSQL
      â†’ Indice Vettoriale (HNSW/IVF-FLAT)
      â†’ /api/chat â†’ embedding domanda â†’ ricerca vettoriale
      â†’ contesto â†’ prompt â†’ modello AI â†’ risposta â†’ JSON
```

---

# 3. Fase 1 â€” Acquisizione e gestione dei file
La fase Ã¨ gestita dal comando:
```
app:index-docs <path>
```

Per ogni file:
1. calcolo dellâ€™hash SHA1,
2. creazione o aggiornamento di **DocumentFile**,
3. rimozione chunk precedenti (garanzia di consistenza),
4. passaggio allâ€™estrazione del testo.

Questa fase garantisce
- idempotenza,
- gestione di file aggiornati,
- nessun duplicato.

---

# 4. Fase 2 â€” Estrazione e normalizzazione del testo
Gestita da **DocumentTextExtractor**.

## Formati supportati
- PDF,
- Markdown,
- DOCX,
- ODT.

## Normalizzazioni applicate
- rimozione emoji, dingbats e simboli non informativi,
- rimozione whitespace multipli,
- rimozione caratteri invisibili,
- correzione degli spazi mancanti dopo segni di punteggiatura,
- correzione separazione MAIUSCOLO+Capitalized (es. "RUOLORisorse" â†’ "RUOLO Risorse"),
- trim globale.

Questa fase produce un testo ideale per il chunking.

---

# 5. Fase 3 â€” Chunking avanzato
Implementato dal **ChunkingService**.

### Caratteristiche principali
- uso di frasi e parole come unitÃ  logiche,
- rispetto limite assoluto HARD_MAX_CHARS (1500),
- fusione chunk finali troppo corti,
- overlap basato sulle ultime parole del chunk precedente,
- divisione ragionata dei paragrafi.

### Parametri consigliati
- `min`: 400â€“500 caratteri
- `target`: 1200â€“1600 caratteri
- `max`: 1500â€“1800 caratteri
- `overlap`: 200â€“300 caratteri

Lâ€™obiettivo: chunk coerenti, completi e ben indicizzabili.

---

# 6. Fase 4 â€” Generazione embedding
Ogni chunk viene trasformato in un vettore float via AiClientInterface.

Modelli supportati:
- OpenAI embedding (1536 dim),
- Ollama embedding (bge-m3, 1024 dim),
- ogni altro modello conforme allo standard.

La dimensionalitÃ  vettoriale Ã¨ dichiarata nel database come:
```
VECTOR(1024)
```
(consigliato, stabile e compatibile con bge-m3).

---

# 7. Fase 5 â€” Persistenza e struttura dati
I chunk e i file vengono memorizzati tramite Doctrine.

## DocumentFile
- UUID
- path
- hash
- createdAt
- relazione OneToMany â†’ chunk

## DocumentChunk
- UUID
- chunkIndex
- contenuto
- embedding vettoriale
- relazione ManyToOne â†’ DocumentFile

---

# 8. Fase 6 â€” Costruzione o aggiornamento dellâ€™indice vettoriale
ELARA supporta entrambi gli indici disponibili in pgvector:

## âœ” HNSW (consigliato)
- piÃ¹ preciso
- piÃ¹ semplice
- piÃ¹ veloce

## âœ” IVF-FLAT (solo dataset enormi)
- richiede tuning `lists` e `probes`
- richiede REINDEX
- meno preciso se configurato male

Il middleware **PgvectorIvfflatMiddleware** si occupa della ricostruzione automatica (quando configurato).

---

# 9. Fase 7 â€” Workflow API /api/chat
Lâ€™API principale Ã¨:
```
POST /api/chat
{
  "message": "domanda utente"
}
```

Flusso:
1. validazione input,
2. chiamata al ChatbotService,
3. gestione eccezioni,
4. generazione risposta JSON.

---

# 10. Fase 8 â€” Retrieval e costruzione del contesto
ChatbotService esegue:
1. embedding della domanda,
2. ricerca tramite cosine distance:
   ```
   ORDER BY embedding <=> :query_vec
   ```
3. applicazione `top_k` (tipicamente 3â€“6 in base al modello embedding),
4. eventuale soglia minima di similaritÃ  (es. > 0.55),
5. costruzione del **contesto RAG**, concatenando i chunk.

### Obiettivo
Recuperare solo i chunk semanticamente piÃ¹ rilevanti.

---

# 11. Fase 9 â€” Generazione della risposta tramite LLM
ChatbotService genera un prompt del tipo:

```
Rispondi SOLO usando il contesto sotto. Non inventare.

# CONTEXT
{{context}}

# DOMANDA
{{question}}
```

Il backend AI (Ollama o OpenAI) produce la risposta, che viene restituita in formato JSON.

---

# 12. Fallback: Test mode & Offline mode
ELARA supporta due modalitÃ  operative aggiuntive.

## Test mode
Attivabile via ENV:
```
APP_AI_TEST_MODE=true
```
Il sistema:
- non chiama lâ€™AI,
- cerca chunk usando LIKE,
- restituisce estratti utili per diagnosticare il retrieval.

## Offline fallback
```
APP_AI_OFFLINE_FALLBACK=true
```
Se il backend AI fallisce, ELARA:
- recupera chunk rilevanti,
- restituisce un riassunto non generato.

---

# 13. Flusso completo riassunto

```
app:index-docs
 â†’ hashed file
 â†’ estrazione testo
 â†’ normalizzazione
 â†’ chunking
 â†’ embedding
 â†’ salvataggio
 â†’ indice vettoriale

/api/chat
 â†’ embedding domanda
 â†’ ricerca vettoriale
 â†’ contesto
 â†’ prompt RAG
 â†’ AI backend
 â†’ risposta JSON
```

---

# 14. Conclusione
Il flusso applicativo di ELARA Ã¨ progettato per essere:
- **robusto**, grazie a fasi chiaramente separate,
- **scalabile**, tramite pgvector e indici personalizzabili,
- **affidabile**, grazie a fallback e test mode,
- **estendibile**, grazie alla modularitÃ  dei servizi AI,
- **deterministico**, poichÃ© risponde solo usando i documenti indicizzati.

Questo documento rappresenta la versione completa e consolidata dellâ€™intero flusso applicativo, pronta per essere utilizzata come riferimento principale nella documentazione tecnica di ELARA.

---
